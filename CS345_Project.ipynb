{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gabriel Bertasius & Jaden Ford#\n",
    "\n",
    "# Predicting Game Success: A Regression Analysis on the Steam Games Dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data into a dataframe for easy handling\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import gzip\n",
    "DATASET_DIR = './data/'\n",
    "DATASET_FILENAME = 'steamgames.parquet'\n",
    "DATASET_PATH = DATASET_DIR+DATASET_FILENAME\n",
    "DATASET_COMPRESSION = 'zstd'  # Very fast and compresses as well as gzip\n",
    "MODELS_DIR = './models/'\n",
    "MODELS_FILENAME = 'model-'\n",
    "download_data = 1\n",
    "\n",
    "\n",
    "def check_file_exists(path: str) -> bool:\n",
    "    return os.path.exists(path)\n",
    "\n",
    "\n",
    "def check_data_dir_exists() -> bool:\n",
    "    return os.path.exists(DATASET_DIR)\n",
    "\n",
    "def check_models_dir_exists() -> bool:\n",
    "    return os.path.exists(MODELS_DIR)\n",
    "\n",
    "def create_data_dir():\n",
    "    directory_name = DATASET_DIR\n",
    "    try:\n",
    "        os.mkdir(directory_name)\n",
    "        print(f\"Directory '{directory_name}' created successfully.\")\n",
    "    except FileExistsError:\n",
    "        print(f\"Directory '{directory_name}' already exists.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: Unable to create '{directory_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def create_models_dir():\n",
    "    directory_name = MODELS_DIR \n",
    "    try:\n",
    "        os.mkdir(directory_name)\n",
    "        print(f\"Directory '{directory_name}' created successfully.\")\n",
    "    except FileExistsError:\n",
    "        print(f\"Directory '{directory_name}' already exists.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: Unable to create '{directory_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def download_steamgames_dataset() -> pd.DataFrame:\n",
    "    df = pd.read_parquet(\n",
    "        \"hf://datasets/FronkonGames/steam-games-dataset/data/train-00000-of-00001-e2ed184370a06932.parquet\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_dataset_pqt(df: pd.DataFrame, filename: str = DATASET_FILENAME, overwrite: bool = False) -> bool:\n",
    "    dir = DATASET_DIR\n",
    "    path = dir+filename\n",
    "    if (check_data_dir_exists() == False):\n",
    "        create_data_dir()\n",
    "    if check_file_exists(path) and overwrite == False:\n",
    "        print(\"File exists. Pass 'overwrite' to replace.\")\n",
    "        return False\n",
    "    else:\n",
    "        df.to_parquet(path, compression='zstd')\n",
    "        return True\n",
    "\n",
    "\n",
    "def read_dataset_pqt(filename: str = DATASET_FILENAME):\n",
    "    path = DATASET_DIR+filename\n",
    "    if check_file_exists(path):\n",
    "        print(\"Loading dataset from local storage...\")\n",
    "        prq = pd.read_parquet(path)\n",
    "        print(\"✅ Local dataset loaded.\")\n",
    "        return prq\n",
    "    else:\n",
    "        print(\"Parquet file not found.\")\n",
    "\n",
    "def datestamp():\n",
    "    \"\"\" Get the current datestamp \"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def write_model_log(path:str, config: dict, **kwargs):\n",
    "    with open(path+\".txt\", \"a\") as file:\n",
    "        file.write(f\"[{datestamp()}]\\n\")\n",
    "        file.write(f\"{config}\\n\")\n",
    "        if kwargs:\n",
    "            for x in kwargs:\n",
    "                file.write(f\"{x}\\n\")\n",
    "    \n",
    "def pickle_model(filename: str, model, params_dict:dict, param_grid:dict = None,overwrite: bool=False, **extra_data):\n",
    "    dir = MODELS_DIR \n",
    "    path = dir+filename\n",
    "    for s in params_dict.values():\n",
    "        path += f'-{s}'\n",
    "    if (check_models_dir_exists() == False):\n",
    "        create_models_dir()\n",
    "    if check_file_exists(path) and overwrite == False:\n",
    "        print(\"File exists. Pass 'overwrite' to replace.\")\n",
    "        return False\n",
    "    else:\n",
    "        if param_grid is not None:\n",
    "            write_model_log(path, param_grid, **extra_data)\n",
    "        else:\n",
    "            write_model_log(path, params_dict)\n",
    "        level = 7   # Good balance between speed and compression\n",
    "        with gzip.open(path+\".pkl.gz\", \"wb\", compresslevel=level) as file:\n",
    "            pickle.dump(model, file, protocol=5)\n",
    "        return True\n",
    "\n",
    "def unpickle_model(filename):\n",
    "    path = MODELS_DIR+filename\n",
    "    with gzip.open(path+\".pkl.gz\", \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "    \n",
    "def download_and_save_dataset(force: bool = False, filename: str = DATASET_FILENAME) -> pd.DataFrame | None:\n",
    "    dir = DATASET_DIR\n",
    "    path = dir+filename\n",
    "    if (check_file_exists(path)):\n",
    "        print(f\"⚠️ Dataset exists locally. Path:{path}\")\n",
    "        if (force == False):\n",
    "            print(\"Use force=True to download and overwrite.\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"Redownloading and Overwriting...\")\n",
    "    else:\n",
    "        print(f\"Downloading and saving dataset to {path} \")\n",
    "    df = download_steamgames_dataset()\n",
    "    write_dataset_pqt(df, overwrite=True)\n",
    "    print(\"✅ Done.\")\n",
    "    print(f\"Saved to: {path}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df = download_and_save_dataset(force=False)\n",
    "if(df is None):\n",
    "    df = read_dataset_pqt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing values\n",
    "sum = df.isnull().sum()\n",
    "sum[sum != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any columns that won't contribute to a game's success rating\n",
    "cols_to_remove = ['About the game', 'Supported languages', 'Full audio languages',\n",
    "                  'Header image', 'Website', 'Support url', 'Support email', 'Metacritic url',\n",
    "                  'Score rank', 'Screenshots', 'Movies']\n",
    "df = df.drop(columns=cols_to_remove, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, fields, field\n",
    "\n",
    "@dataclass\n",
    "class DataMinMax:\n",
    "    data:dict = field(default_factory=dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that calculates the number of years since a game's release date\n",
    "from datetime import datetime\n",
    "def years_since_release(date_string):\n",
    "  if len(date_string) == 11 or len(date_string) == 12:\n",
    "        date = datetime.strptime(date_string, \"%b %d, %Y\")\n",
    "  else: # length must be 8 or 9\n",
    "      date = datetime.strptime(date_string, \"%b %Y\")\n",
    "\n",
    "  current_date = datetime.now()\n",
    "  years = (current_date - date).days / 365\n",
    "  return years\n",
    "\n",
    "# function to return the avg number of estimated owners\n",
    "def est_owners(num_owners):\n",
    "  numbers = num_owners.split('-')\n",
    "  return (int(numbers[0]) + int(numbers[1])) / 2\n",
    "\n",
    "# function to normalize a numerical column between 0-1 based on min and and max values\n",
    "def min_max_normalize(column):\n",
    "  column = np.array(column)\n",
    "  norm_col = ( column - np.min(column) ) / ( np.max(column) - np.min(column) )\n",
    "  return norm_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert release date to years since release\n",
    "df['Release date'] = df['Release date'].apply(years_since_release)\n",
    "\n",
    "# return middle value for each given range of estimated owners\n",
    "df['Estimated owners'] = df['Estimated owners'].apply(est_owners)\n",
    "\n",
    "# convert windows, mac, and linux columns from boolean to integer\n",
    "df['Windows'] = df['Windows'].astype(int)\n",
    "df['Mac'] = df['Mac'].astype(int)\n",
    "df['Linux'] = df['Linux'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out any games that are free, have no peak ccu, and no estimated owners\n",
    "# This allows us to judge success based on games that competed in certain markets, and have had actual people play them\n",
    "no_peak_ccu_cols = df[df['Peak CCU'] == 0].index\n",
    "df = df.drop(no_peak_ccu_cols, axis=0)\n",
    "\n",
    "no_est_owners_cols = df[df['Estimated owners'] == 0].index\n",
    "df = df.drop(no_est_owners_cols, axis=0)\n",
    "\n",
    "no_price_cols = df[df['Price'] == 0].index\n",
    "df = df.drop(no_price_cols, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a copy of pre_normalized values\n",
    "\n",
    "df_orig = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize any large value ranges\n",
    "cols_to_normalize = ['Release date', 'Estimated owners', 'Peak CCU', 'Required age', 'Price', 'DLC count',\n",
    "                     'Metacritic score', 'User score', 'Positive', 'Negative', 'Achievements',\n",
    "                     'Recommendations', 'Average playtime forever', 'Average playtime two weeks',\n",
    "                     'Median playtime forever', 'Median playtime two weeks']\n",
    "for col in cols_to_normalize:\n",
    "  df[col] = min_max_normalize(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to remove rows that have no reviews, we would have 4269 examples\n",
    "#df = df.dropna(axis=0, subset='Reviews')\n",
    "#print(df.shape[0])\n",
    "#df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting unique words in Categories, Genres, Tags\n",
    "\n",
    "'Dumb counting' as in the tags 'turn-based' and 'turn-based combat' or 'turn-based strategy' are different words. These should be ok for word2vec as they're similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df['Tags']\n",
    "\n",
    "def count_unique_words(df, label:str):\n",
    "    lists:pd.Series= df[label].str.casefold().str.split(',')\n",
    "    words = set()\n",
    "    [words.update(x) for x in lists if x is not None]\n",
    "    print(f\"Number of unique {label}: {len(words)}\")\n",
    "    return words\n",
    "\n",
    "count_unique_words(df, 'Categories') # 39\n",
    "count_unique_words(df, 'Genres') # 27\n",
    "count_unique_words(df, 'Tags') # 444\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_categories = df['Categories'].str.get_dummies(sep=',')\n",
    "encoded_genres = df['Genres'].str.get_dummies(sep=',')\n",
    "\n",
    "df = pd.concat([df, encoded_categories, encoded_genres], axis=1)\n",
    "df = df.drop(columns=['Categories', 'Genres'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec embedding for Tags feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the embedding for the tags is an average of the tags for a given game. This results in d-dimensional feature embedding where d is the numer of dimensions specified in word2vec training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: process hyphenated and multi-word tags. Treat as one phrase by subbing dashes and spaces with an underline\n",
    "\n",
    "todo: tuning: what do the parameters do? what can be tweaked? what is desired?\n",
    "\n",
    "todo: CBOW vs CSkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'Tags'\n",
    "lists:pd.Series= df[label].str.casefold().str.split(',')\n",
    "# lists.fillna('none')\n",
    "lists = lists.apply(lambda x: ['none'] if x is None else x)\n",
    "sentences = [x for x in lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model_name = \"100features_1minwords_10context\"\n",
    "model = None\n",
    "\n",
    "num_features = 100  # Word vector dimensionality\n",
    "min_word_count = 1  # Minimum word count\n",
    "num_workers = 8  # Number of threads to run in parallel\n",
    "context = 10  # Context window size\n",
    "downsampling = 1e-3  # Downsample setting for frequent words\n",
    "\n",
    "def init_sims(model):\n",
    "    # If you don't plan to train the model any further, calling\n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    print(\"get_mean_vector is deprecated. Use get_vector(key, norm=True) instead\")\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "\n",
    "if check_file_exists(model_name):\n",
    "    \"\"\"Normalize Vectors\"\"\"\n",
    "    print(\"Loading saved model\")\n",
    "    model = gensim.models.Word2Vec.load(model_name)\n",
    "    init_sims(model)\n",
    "\n",
    "else:\n",
    "\n",
    "    # def process_tags(df: pd.DataFrame):\n",
    "    # model = gensim.models.Word2Vec\n",
    "\n",
    "    # Code from:\n",
    "\n",
    "    # https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview\n",
    "    # Set values for various parameters\n",
    "\n",
    "    print(\"Training model...\")\n",
    "    model = gensim.models.Word2Vec(\n",
    "        sentences,\n",
    "        workers=num_workers,\n",
    "        vector_size=num_features,\n",
    "        min_count=min_word_count,\n",
    "        window=context,\n",
    "        sample=downsampling,\n",
    "    )\n",
    "\n",
    "    # It can be helpful to create a meaningful model name and\n",
    "    # save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector of the tag 'singleplayer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.wv.index_to_key))\n",
    "print(model.wv.index_to_key[3])\n",
    "model.wv['action']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word relative rank (cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('action', topn=10)\n",
    "model.wv.similar_by_word('action', topn=10) # same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)\n",
    "words = model.wv.index_to_key\n",
    "words[0:10]\n",
    "model.wv.most_similar('none') # this needs fixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    pre-normalizing will discard sentence length information\n",
    "    this should ignore differences in numbe of tags specified for each game\n",
    "    Pre-normalize doesnt matter if init_sims(replace=True) since it will\n",
    "    precompute normalized vectors.\n",
    "    Not clear what the point of post_normalize is. May be/not good for training\n",
    "    the regression model down the line.\n",
    "\"\"\"\n",
    "\n",
    "tags_vectors = [\n",
    "    model.wv.get_mean_vector(game, pre_normalize=False, post_normalize=False)\n",
    "    for game in sentences\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of games', len(tags_vectors))\n",
    "tags_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vdf = pd.DataFrame(tags_vectors)\n",
    "assert w2vdf.shape[1] == num_features\n",
    "w2vdf.columns = [f'w2v_embed_{i}' for i in range(num_features)]\n",
    "w2vdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping Tags columns and merging embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Trick to prevent this from executing twice \"\"\"\n",
    "try:\n",
    "    check_if_w2vdf_already_concat\n",
    "except NameError:\n",
    "    df.drop(columns=['Tags'])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df, w2vdf], axis=1)\n",
    "    check_if_w2vdf_already_concat = 1\n",
    "\n",
    "# del check_if_w2vdf_already_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test data extraction + Regression model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important metrics when determinng a game's success include the number of estimated owners, peak ccu, number of pos/neg reveiws, and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df[['Estimated owners', 'Peak CCU', 'Positive', 'Negative', 'Price']])\n",
    "X = np.array(df.drop(columns=['AppID', 'Name', 'Estimated owners', 'Peak CCU', 'Positive', 'Negative', 'Price', 'Reviews', 'Notes', 'Developers', 'Publishers', 'Tags'], axis=1))\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X[0,:]) # ensure all data is numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestRegressor is used to handle non-linear relationships between a game and the metrics we are predicting. MultiOutputRegressor provides easier setup for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grid search will also be done on the hyperparemeters for the random forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 70% training data, 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer, this cell takes hours to complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_search = False\n",
    "grid_search = None\n",
    "if perform_search == True:\n",
    "      # perform a grid search on hyperparameters for random forest\n",
    "      # -1 to utilize all processors and speed up training time\n",
    "      rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "      model = MultiOutputRegressor(rf, n_jobs=-1)\n",
    "\n",
    "      param_grid = [\n",
    "      {'estimator__n_estimators': [20, 50, 100, 150, 200, 250],\n",
    "       'estimator__max_features': [1, 20, 'sqrt', 50, 70, 90, 110],\n",
    "       'estimator__max_depth': [None, 10, 20, 30, 40, 50]}\n",
    "      ]\n",
    "\n",
    "      grid_search = GridSearchCV(model, param_grid, n_jobs=-1)\n",
    "      grid_search.fit(X_train, y_train)\n",
    "\n",
    "      pickle_model(\"rf_gridsearch_obj\", grid_search, grid_search.best_params_, param_grid[0])\n",
    "      print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Outdated*\\\n",
    "Best hyperparams were no max branch depth, 50 features, and 100 estimators for random forest. These parameters are the most infuential to model capacity, generalization, and computation. Other parameters like min_samples_split were ommitted from grid search since the default is adequte to recognize patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "if perform_search == True:\n",
    "    model = grid_search.best_estimator_\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred, multioutput='raw_values')\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2_score_values= r2_score(y_test, y_pred, multioutput='raw_values')\n",
    "\n",
    "    # ['Estimated owners', 'Peak CCU', 'Positive', 'Negative', 'Price']\n",
    "    print(\"Test set Mean Squared Error:\", mse)\n",
    "    print(\"Test set Root Mean Squared Error:\", rmse)\n",
    "    print(\"Test set R2 Score:\", r2_score_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model\n",
    "\"\"\" You can download from link in README \"\"\"\n",
    "loaded_grid = unpickle_model(\"rf_gridsearch_obj-50-70-150\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_labels = [\"Estimated owners\", \"Peak CCU\", \"Positive\", \"Negative\", \"Price\"]\n",
    "calc: DataMinMax = DataMinMax()\n",
    "for i, label in enumerate(predict_labels):\n",
    "    calc.data[label] = {\n",
    "        \"min\": df_orig[label].min(),\n",
    "        \"max\": df_orig[label].max(),\n",
    "        \"r2\": r2_score_values[i],\n",
    "        \"rmse\": rmse[i],\n",
    "    }\n",
    "# owners = df_orig['Estimated owners']\n",
    "# calc.owners = (min())\n",
    "# du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into a DataFrame\n",
    "rows = []\n",
    "for label in predict_labels:\n",
    "\n",
    "    fmt_int = lambda x: f\"{x:,.0f}\"\n",
    "    fmt_float = lambda x, precision=2: f\"{x:,.{precision}f}\"\n",
    "    min_val = calc.data[label][\"min\"]\n",
    "    max_val = calc.data[label][\"max\"]\n",
    "    rmse_val = calc.data[label][\"rmse\"]\n",
    "    range_val = max_val - min_val\n",
    "    range_percent = rmse_val * 100\n",
    "    prediction = rmse_val*range_val\n",
    "    rows.append({\n",
    "        \"Metric\": label,\n",
    "        \"Prediction\": fmt_float(prediction, 2),\n",
    "        \"Min\": fmt_float(min_val,2),\n",
    "        \"Max\": fmt_float(max_val),\n",
    "        \"RMSE\": fmt_float(rmse_val, 4),\n",
    "        \"Range (%)\": fmt_float(range_percent,2),\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df_result = pd.DataFrame(rows)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set Mean Squared Error: [5.53550695e-05 1.01822779e-05 3.49412680e-05 1.67333804e-04\n",
    " 1.09633784e-03]\n",
    "\n",
    "Test set Root Mean Squared Error: [0.0074401  0.00319097 0.00591111 0.01293576 0.03311099]\n",
    "\n",
    "Test set R2 Score: [0.76685888 0.31981091 0.94586848 0.62299915 0.46089451]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the R2 Score, which indicates the proportion of variance in the dependent variable that is predictable from the independent variables, the model is able to capture underlying patterns decently for the estimated owners, positive number of reviews, and negative number of reviews. This suggests that relationships between the features and target variables are relatively strong, making them easier to predict.\n",
    "\n",
    "This is logical. Game characteristics like developers, publishers, and categories will directly influence price and peak ccu  counts more so than the other target variables. Since these aren't taken into account during training to avoid too many feature encodings, the correlation between these characteristics makes them harder to predict. **This will help us assign a score to each prediction when defining a success rating.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the equation: **original_value = (normalized_value * (data_max - data_min)) + data_min**, let's calculate the rmse for each predicted value in the original data scale to gauge how good/bad the mse is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Max Values:                \n",
    "*   75,000,000 owners                      \n",
    "*   872,138 CCU                            \n",
    "*   964,983 Positive Reviews                 \n",
    "*   138,530 Negative Reviews                    \n",
    "*   $ 269.99                             \n",
    "\n",
    "\n",
    "Original Min Values:\n",
    "*   10,000 owners\n",
    "*   1 CCU\n",
    "*   0  Positive Reviews\n",
    "*   0  Negative Reviews\n",
    "*   $ 0.6456165345712968\n",
    "\n",
    "**range % determined by rsme/(max-min)**\n",
    "\n",
    "RMSE in original data range:\n",
    "*   567,933.0055858027 owners  (0.76% of the range)\n",
    "*   2,783.9614091764875 CCU (0.32% of range)\n",
    "*   5,704.1244660345665  Positive Reviews (0.59% of range)\n",
    "*   1,791.9903878277946  Negative Reviews (1.3% of range)\n",
    "*   $ 9.278048072328264 (3.4% of range)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1CD4EsTokInoL8rrk7tV9dBVteDf4sZ2l",
     "timestamp": 1731559683460
    }
   ]
  },
  "kernelspec": {
   "display_name": "steamgames",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
