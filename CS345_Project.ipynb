{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gabriel Bertasius & Jaden Ford#\n",
    "\n",
    "# Predicting Game Success: A Regression Analysis on the Steam Games Dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data into a dataframe for easy handling\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import gzip\n",
    "DATASET_DIR = './data/'\n",
    "DATASET_FILENAME = 'steamgames.parquet'\n",
    "DATASET_PATH = DATASET_DIR+DATASET_FILENAME\n",
    "DATASET_COMPRESSION = 'zstd'  # Very fast and compresses as well as gzip\n",
    "MODELS_DIR = './models/'\n",
    "MODELS_FILENAME = 'model-'\n",
    "download_data = 1\n",
    "\n",
    "\n",
    "def check_file_exists(path: str) -> bool:\n",
    "    return os.path.exists(path)\n",
    "\n",
    "\n",
    "def check_data_dir_exists() -> bool:\n",
    "    return os.path.exists(DATASET_DIR)\n",
    "\n",
    "def check_models_dir_exists() -> bool:\n",
    "    return os.path.exists(MODELS_DIR)\n",
    "\n",
    "def create_data_dir():\n",
    "    directory_name = DATASET_DIR\n",
    "    try:\n",
    "        os.mkdir(directory_name)\n",
    "        print(f\"Directory '{directory_name}' created successfully.\")\n",
    "    except FileExistsError:\n",
    "        print(f\"Directory '{directory_name}' already exists.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: Unable to create '{directory_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def create_models_dir():\n",
    "    directory_name = MODELS_DIR\n",
    "    try:\n",
    "        os.mkdir(directory_name)\n",
    "        print(f\"Directory '{directory_name}' created successfully.\")\n",
    "    except FileExistsError:\n",
    "        print(f\"Directory '{directory_name}' already exists.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: Unable to create '{directory_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def download_steamgames_dataset() -> pd.DataFrame:\n",
    "    df = pd.read_parquet(\n",
    "        \"hf://datasets/FronkonGames/steam-games-dataset/data/train-00000-of-00001-e2ed184370a06932.parquet\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_dataset_pqt(df: pd.DataFrame, filename: str = DATASET_FILENAME, overwrite: bool = False) -> bool:\n",
    "    dir = DATASET_DIR\n",
    "    path = dir+filename\n",
    "    if (check_data_dir_exists() == False):\n",
    "        create_data_dir()\n",
    "    if check_file_exists(path) and overwrite == False:\n",
    "        print(\"File exists. Pass 'overwrite' to replace.\")\n",
    "        return False\n",
    "    else:\n",
    "        df.to_parquet(path, compression='zstd')\n",
    "        return True\n",
    "\n",
    "\n",
    "def read_dataset_pqt(filename: str = DATASET_FILENAME):\n",
    "    path = DATASET_DIR+filename\n",
    "    if check_file_exists(path):\n",
    "        print(\"Loading dataset from local storage...\")\n",
    "        prq = pd.read_parquet(path)\n",
    "        print(\"✅ Local dataset loaded.\")\n",
    "        return prq\n",
    "    else:\n",
    "        print(\"Parquet file not found.\")\n",
    "\n",
    "def datestamp():\n",
    "    \"\"\" Get the current datestamp \"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def write_model_log(path:str, config: dict, **kwargs):\n",
    "    with open(path+\".txt\", \"a\") as file:\n",
    "        file.write(f\"[{datestamp()}]\\n\")\n",
    "        file.write(f\"{config}\\n\")\n",
    "        if kwargs:\n",
    "            for x in kwargs:\n",
    "                file.write(f\"{x}\\n\")\n",
    "\n",
    "def pickle_model(filename: str, model, params_dict:dict, param_grid:dict = None,overwrite: bool=False, **extra_data):\n",
    "    dir = MODELS_DIR\n",
    "    path = dir+filename\n",
    "    for s in params_dict.values():\n",
    "        path += f'-{s}'\n",
    "    if (check_models_dir_exists() == False):\n",
    "        create_models_dir()\n",
    "    if check_file_exists(path) and overwrite == False:\n",
    "        print(\"File exists. Pass 'overwrite' to replace.\")\n",
    "        return False\n",
    "    else:\n",
    "        if param_grid is not None:\n",
    "            write_model_log(path, param_grid, **extra_data)\n",
    "        else:\n",
    "            write_model_log(path, params_dict)\n",
    "        level = 7   # Good balance between speed and compression\n",
    "        with gzip.open(path+\".pkl.gz\", \"wb\", compresslevel=level) as file:\n",
    "            pickle.dump(model, file, protocol=5)\n",
    "        return True\n",
    "\n",
    "def unpickle_model(filename):\n",
    "    path = MODELS_DIR+filename\n",
    "    with gzip.open(path+\".pkl.gz\", \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "def download_and_save_dataset(force: bool = False, filename: str = DATASET_FILENAME) -> pd.DataFrame | None:\n",
    "    dir = DATASET_DIR\n",
    "    path = dir+filename\n",
    "    if (check_file_exists(path)):\n",
    "        print(f\"⚠️ Dataset exists locally. Path:{path}\")\n",
    "        if (force == False):\n",
    "            print(\"Use force=True to download and overwrite.\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"Redownloading and Overwriting...\")\n",
    "    else:\n",
    "        print(f\"Downloading and saving dataset to {path} \")\n",
    "    df = download_steamgames_dataset()\n",
    "    write_dataset_pqt(df, overwrite=False)\n",
    "    print(\"✅ Done.\")\n",
    "    print(f\"Saved to: {path}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df = download_and_save_dataset(force=False)\n",
    "if(df is None):\n",
    "    df = read_dataset_pqt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing values\n",
    "sum = df.isnull().sum()\n",
    "sum[sum != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any columns that won't contribute to a game's success rating\n",
    "cols_to_remove = ['About the game', 'Supported languages', 'Full audio languages',\n",
    "                  'Header image', 'Website', 'Support url', 'Support email', 'Metacritic url',\n",
    "                  'Score rank', 'Screenshots', 'Movies']\n",
    "df = df.drop(columns=cols_to_remove, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing data and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that calculates the number of years since a game's release date\n",
    "from datetime import datetime\n",
    "def years_since_release(date_string):\n",
    "  if len(date_string) == 11 or len(date_string) == 12:\n",
    "        date = datetime.strptime(date_string, \"%b %d, %Y\")\n",
    "  else: # length must be 8 or 9\n",
    "      date = datetime.strptime(date_string, \"%b %Y\")\n",
    "\n",
    "  current_date = datetime.now()\n",
    "  years = (current_date - date).days / 365\n",
    "  return years\n",
    "\n",
    "# function to return the avg number of estimated owners\n",
    "def est_owners(num_owners):\n",
    "  numbers = num_owners.split('-')\n",
    "  return (int(numbers[0]) + int(numbers[1])) / 2\n",
    "\n",
    "# function to normalize a numerical column between 0-1 based on min and and max values\n",
    "def min_max_normalize(column):\n",
    "  column = np.array(column)\n",
    "  norm_col = ( column - np.min(column) ) / ( np.max(column) - np.min(column) )\n",
    "  return norm_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert release date to years since release\n",
    "df['Release date'] = df['Release date'].apply(years_since_release)\n",
    "\n",
    "# return middle value for each given range of estimated owners\n",
    "df['Estimated owners'] = df['Estimated owners'].apply(est_owners)\n",
    "\n",
    "# convert windows, mac, and linux columns from boolean to integer\n",
    "df['Windows'] = df['Windows'].astype(int)\n",
    "df['Mac'] = df['Mac'].astype(int)\n",
    "df['Linux'] = df['Linux'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out any games that are free, have no peak ccu, and no estimated owners\n",
    "# This allows us to judge success based on games that competed in certain markets, and have had actual people play them\n",
    "no_peak_ccu_cols = df[df['Peak CCU'] == 0].index\n",
    "df = df.drop(no_peak_ccu_cols, axis=0)\n",
    "\n",
    "no_est_owners_cols = df[df['Estimated owners'] == 0].index\n",
    "df = df.drop(no_est_owners_cols, axis=0)\n",
    "\n",
    "no_price_cols = df[df['Price'] == 0].index\n",
    "df = df.drop(no_price_cols, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for later use in sentiment analysis and model performance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a copy of pre_normalized values\n",
    "df_orig = df.copy(deep=True)\n",
    "\n",
    "# store reviews for sentiment analysis\n",
    "df_reviews = df['Reviews'].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize any large value ranges\n",
    "cols_to_normalize = ['Release date', 'Estimated owners', 'Peak CCU', 'Required age', 'Price', 'DLC count',\n",
    "                     'Metacritic score', 'User score', 'Positive', 'Negative', 'Achievements',\n",
    "                     'Recommendations', 'Average playtime forever', 'Average playtime two weeks',\n",
    "                     'Median playtime forever', 'Median playtime two weeks']\n",
    "for col in cols_to_normalize:\n",
    "  df[col] = min_max_normalize(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to remove rows that have no reviews, we would have 4269 examples\n",
    "#df = df.dropna(axis=0, subset='Reviews')\n",
    "#print(df.shape[0])\n",
    "#df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting unique words in Categories, Genres, Tags\n",
    "\n",
    "'Dumb counting' as in the tags 'turn-based' and 'turn-based combat' or 'turn-based strategy' are different words. These should be ok for word2vec as they're similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df['Tags']\n",
    "\n",
    "def count_unique_words(df, label:str):\n",
    "    lists:pd.Series= df[label].str.casefold().str.split(',')\n",
    "    words = set()\n",
    "    [words.update(x) for x in lists if x is not None]\n",
    "    print(f\"Number of unique {label}: {len(words)}\")\n",
    "    return len(words)\n",
    "\n",
    "count_unique_words(df, 'Categories') # 39\n",
    "count_unique_words(df, 'Genres') # 27\n",
    "count_unique_words(df, 'Tags') # 444\n",
    "pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding Catergories and Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_categories = df['Categories'].str.get_dummies(sep=',')\n",
    "encoded_genres = df['Genres'].str.get_dummies(sep=',')\n",
    "\n",
    "df = pd.concat([df, encoded_categories, encoded_genres], axis=1)\n",
    "df = df.drop(columns=['Categories', 'Genres'], axis=1)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec embedding for Tags feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the embedding for the tags is an average of the tags for a given game. This results in d-dimensional feature embedding where d is the numer of dimensions specified in word2vec training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: process hyphenated and multi-word tags. Treat as one phrase by subbing dashes and spaces with an underline\n",
    "\n",
    "todo: tuning: what do the parameters do? what can be tweaked? what is desired?\n",
    "\n",
    "todo: CBOW vs CSkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'Tags'\n",
    "lists:pd.Series= df[label].str.casefold().str.split(',')\n",
    "# lists.fillna('none')\n",
    "lists = lists.apply(lambda x: ['none'] if x is None else x)\n",
    "sentences = [x for x in lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "class s_word2vec:\n",
    "# model_name = \"100features_1minwords_10context\"\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.num_features = 100# Word vector dimensionality\n",
    "        self.min_word_count = 1  # Minimum word count\n",
    "        self.context = 10 # Context window size\n",
    "        self.model_name = f'{self.num_features}-feat_{self.min_word_count}-minwords_{self.context}-context'\n",
    "\n",
    "        self.num_workers = 8  # Number of threads to run in parallel\n",
    "        self.downsampling = 1e-3  # Downsample setting for frequent words\n",
    "\n",
    "    def _init_sims(self, model):\n",
    "        # If you don't plan to train the model any further, calling\n",
    "        # init_sims will make the model much more memory-efficient.\n",
    "        print(\"get_mean_vector is deprecated. Use get_vector(key, norm=True) instead\")\n",
    "        self.model.init_sims(replace=True)\n",
    "\n",
    "    def load_or_train_model(self):\n",
    "        if check_file_exists(self.model_name):\n",
    "            print(\"Loading saved model: \", self.model_name)\n",
    "            self.model = gensim.models.Word2Vec.load(self.model_name)\n",
    "            self._init_sims(self.model)\n",
    "\n",
    "        else:\n",
    "            # Code from:\n",
    "            # https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview\n",
    "\n",
    "            print(\"Training model...\")\n",
    "            self.model = gensim.models.Word2Vec(\n",
    "                sentences,\n",
    "                workers=self.num_workers,\n",
    "                vector_size=self.num_features,\n",
    "                min_count=self.min_word_count,\n",
    "                window=self.context,\n",
    "                sample=self.downsampling,\n",
    "            )\n",
    "            # It can be helpful to create a meaningful model name and\n",
    "            # save the model for later use. You can load it later using Word2Vec.load()\n",
    "            self.model.save(self.model_name)\n",
    "\n",
    "\n",
    "tags_w2v_model = s_word2vec()\n",
    "tags_w2v_model.load_or_train_model()\n",
    "model:gensim.models.Word2Vec = tags_w2v_model.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector of the tag 'singleplayer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = model.wv.get_mean_vector(sentences[9][1:])\n",
    "model.wv.similar_by_vector(sent, topn=len(sentences[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.wv.index_to_key))\n",
    "print(model.wv.index_to_key[3])\n",
    "model.wv['action'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing word2vec tag vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TSNE we can visualize the clustering of similar word vectors in word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "# conda install adjustText::conda-forge\n",
    "from adjustText import adjust_text\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\"\"\" Code for graphing from:\n",
    "    https://github.com/arsena-k/Word2Vec-bias-extraction/blob/master/Part_A_W2V_training_performance_exploring.ipynb\n",
    "\"\"\"\n",
    "def tsne_plot(words, vectors, iterations, seed, title):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    # you may need to tune these, epsecially the perplexity.\n",
    "    tsne_model = TSNE(\n",
    "        perplexity=7,\n",
    "        n_components=2,\n",
    "        init=\"pca\",\n",
    "        max_iter=iterations,\n",
    "        random_state=seed,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    new_values = tsne_model.fit_transform(np.asarray(vectors))\n",
    "    # pca = PCA(2, svd_solver='full', random_state=42)\n",
    "    # new_values = pca.fit_transform(np.asarray(vectors))\n",
    "\n",
    "    x,y, texts = [],[],[]\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i], y[i])\n",
    "        texts.append( plt.text(\n",
    "                s=words[i],\n",
    "                #  xy=(x[i], y[i]),\n",
    "                x=x[i], y=y[i],\n",
    "                #  xytext=(x[i] + 0.1, y[i] - 0.2),\n",
    "                #   xytext=(5, 2),\n",
    "                #  textcoords='offset points',\n",
    "                ha=\"center\", va=\"center\",))\n",
    "    adjust_text(\n",
    "        texts,\n",
    "        expand=(6,5),\n",
    "        explode_radius=(15),\n",
    "        avoid_self=False,\n",
    "        max_move=(13,13),\n",
    "        force_text=(4,5),\n",
    "        force_explode=(5,5),\n",
    "        # force_static=(10,15),\n",
    "        # pull_threshold=20,\n",
    "        # force_pull=(0.1,0.1),\n",
    "        arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=0.08\"),\n",
    "    )\n",
    "    plt.ylabel(\"Latent Dimension 1\")\n",
    "    plt.xlabel(\"Latent Dimension 2\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "my_word_list, my_word_vectors, label = [], [], []\n",
    "\n",
    "for i in model.wv.index_to_key:\n",
    "    try:\n",
    "        if my_word_list not in my_word_list:\n",
    "            my_word_vectors.append(model.wv[i])\n",
    "            my_word_list.append(i)\n",
    "    except (\n",
    "        KeyError\n",
    "    ):  # if one of the words_to_explore is not in the model vocab, just skip it\n",
    "        continue\n",
    "\n",
    "tsne_plot(my_word_list, my_word_vectors, iterations=2000, seed=23, title=\"TSNE Visualization of Word-Vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averging word2vec vectors\n",
    "The tag vectors corresponding to each game from the word2vec model are averaged to prepare a 100 dimensional embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('action', topn=10)\n",
    "model.wv.similar_by_word('action', topn=10) # same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = model.wv.index_to_key\n",
    "words[0:10]\n",
    "model.wv.most_similar('none') # this needs fixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    pre-normalizing will discard sentence length information\n",
    "    this should ignore differences in numbe of tags specified for each game\n",
    "    Pre-normalize doesnt matter if init_sims(replace=True) since it will\n",
    "    precompute normalized vectors.\n",
    "    Not clear what the point of post_normalize is. May be/not good for training\n",
    "    the regression model down the line.\n",
    "\"\"\"\n",
    "\n",
    "tags_vectors = [\n",
    "    model.wv.get_mean_vector(game, pre_normalize=False, post_normalize=False)\n",
    "    for game in sentences\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of games', len(tags_vectors))\n",
    "# model.wv.similar_by_vector(tags_vectors[0],topn=20)\n",
    "# np.mean(tags_vectors[0])\n",
    "# np.linalg.norm(tags_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tags'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vdf = pd.DataFrame(tags_vectors)\n",
    "assert w2vdf.shape[1] == tags_w2v_model.num_features\n",
    "w2vdf.columns = [f'w2v_embed_{i}' for i in range(tags_w2v_model.num_features)]\n",
    "w2vdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping Tags columns and merging embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Trick to prevent this from executing twice \"\"\"\n",
    "try:\n",
    "    check_if_w2vdf_already_concat\n",
    "except NameError:\n",
    "    df.drop(columns=['Tags'])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df, w2vdf], axis=1)\n",
    "    check_if_w2vdf_already_concat = 1\n",
    "\n",
    "# del check_if_w2vdf_already_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test = train_test_split(tags_vectors, test_size=0.2, train_size=0.8, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Reviews Using Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.notna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4269 reveiws which we can analyze for sentiment. Using the Twitter Roberta model we get three scores (negative, neutral, positive) which are computed into a compound score using a simple weighting of [-1, 0, 1], respectively, and a dot product of the scores. These scores are then gathered and averaged into a single score for each game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please unzip the sentiment model from the google drive folder into the models directory.\n",
    "\n",
    "When unzipped, the models directory should contain the folder `twitter-roberta-base-sentiment-latest` with 5 files inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q scipy\n",
    "!pip install torch -q torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "\n",
    "\n",
    "def calculate_sentiment():\n",
    "    roberta_path = 'models/twitter-roberta-base-sentiment-latest'\n",
    "    MODEL = roberta_path  # f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    config = AutoConfig.from_pretrained(MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL);\n",
    "\n",
    "\n",
    "    def preprocess(text: str):\n",
    "        if (text.find(\"“\") != -1):\n",
    "            p = text.split(\"“\")\n",
    "            p = [x.split(\"”\")[0].strip() for x in p]\n",
    "            p = p[1:]\n",
    "        else:\n",
    "            # if it's just a single review or follows a different format\n",
    "            # then just treat the whole string as a review\n",
    "            p = [text]\n",
    "        return p\n",
    "\n",
    "    def calculate_compound_score(scores):\n",
    "        sentiment_probabilities = np.asarray(scores)\n",
    "        weights = np.array([-1, 0, 1], dtype=np.float32)\n",
    "        return np.dot(sentiment_probabilities, weights)\n",
    "\n",
    "\n",
    "    s = df_reviews[df_reviews.notna()]\n",
    "    s = s.apply(preprocess)\n",
    "    # Tweak batch to your system.\n",
    "    # Mem Usage: 10-50 is safe. ~25 may be fastest. Needs around 8gb ram.\n",
    "    #=========\n",
    "    batch_size = 25\n",
    "    #=========\n",
    "    scores = []\n",
    "    for i in range(0, s.size, batch_size): # compute in batches\n",
    "        p = s[i:i + batch_size]\n",
    "        pretokenized = [review for row in p.tolist() for review in row] # create a list of reviews\n",
    "        # pretokenized = \"New WW2 Strategy Game Offers A Harrowing Look At Poland's Ill-Fated 1944 Uprising\"\n",
    "        # compute the tokens\n",
    "        encoded_input = tokenizer(\n",
    "            pretokenized, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded_input)\n",
    "        # gather scores in each batch\n",
    "        scores.extend([calculate_compound_score(softmax(logits.numpy()))\n",
    "                      for logits in output.logits])\n",
    "\n",
    "\n",
    "    row_lengths = [len(reviews) for reviews in s.tolist()]\n",
    "    row_lengths\n",
    "    game_sentiments = np.split(scores, np.cumsum(row_lengths)) # review scores grouped by game\n",
    "\n",
    "    if(game_sentiments[-1].shape == (0,)):\n",
    "        game_sentiments = game_sentiments[:-1]\n",
    "\n",
    "    game_sentiments = [np.mean(x) for x in game_sentiments] # average the score for each game\n",
    "\n",
    "    df_review_scores = pd.Series(game_sentiments, index=s.index) # re-index\n",
    "\n",
    "    # Copy df_reviews to avoid overwriting\n",
    "    df_reviews_with_scores = df_reviews.copy()\n",
    "\n",
    "    # Assign scores to the corresponding indices in the new series\n",
    "    df_reviews_with_scores.loc[df_review_scores.index] = df_review_scores\n",
    "\n",
    "    print(df_reviews_with_scores)\n",
    "    return df_reviews_with_scores\n",
    "\n",
    "try:\n",
    "    df_reviews_with_scores = unpickle_model('df_reviews_with_scores-values')\n",
    "except:\n",
    "    print('No stored values found. Running fresh sentiment analysis.')\n",
    "    df_reviews_with_scores = calculate_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_model('df_reviews_with_scores', df_reviews_with_scores, {'no':'values'})\n",
    "print(df_reviews_with_scores.shape)\n",
    "df_reviews_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave any games with no reviews with a neutral sentiment rating (0)\n",
    "df_reviews_with_scores = df_reviews_with_scores.fillna(0)\n",
    "df_reviews_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_reviews_with_scores[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test data extraction + Regression model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important metrics when determinng a game's success include the number of estimated owners, peak ccu, number of pos/neg reveiws, and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df[['Estimated owners', 'Peak CCU', 'Positive', 'Negative', 'Price']])\n",
    "X = np.array(df.drop(columns=['AppID', 'Name', 'Estimated owners', 'Peak CCU', 'Positive', 'Negative', 'Price', 'Reviews', 'Notes', 'Developers', 'Publishers', 'Tags'], axis=1))\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X[0,:]) # ensure all data is numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestRegressor is used to handle non-linear relationships between a game and the metrics we are predicting. MultiOutputRegressor provides easier setup for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grid search will also be done on the hyperparemeters for the random forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 70% training data, 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=52)\n",
    "X_train = pca.fit_transform(X_train_scaled)\n",
    "X_test = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Find the number of components for 87.5% variance\n",
    "n_components = np.argmax(cumulative_variance >= 0.875) + 1  # Add 1 because index starts at 0\n",
    "\n",
    "print(f\"Number of components to preserve 87.5% variance: {n_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer, this cell takes hours to complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_search = False\n",
    "grid_search = None\n",
    "if perform_search == True:\n",
    "      # perform a grid search on hyperparameters for random forest\n",
    "      # -1 to utilize all processors and speed up training time\n",
    "      rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "      model = MultiOutputRegressor(rf, n_jobs=-1)\n",
    "\n",
    "      param_grid = [\n",
    "      {'estimator__n_estimators': [20, 50, 100, 150, 200, 250],\n",
    "       'estimator__max_features': [1, 20, 'sqrt', 50, 70, 90, 110],\n",
    "       'estimator__max_depth': [None, 10, 20, 30, 40, 50]}\n",
    "      ]\n",
    "\n",
    "      grid_search = GridSearchCV(model, param_grid, n_jobs=-1)\n",
    "      grid_search.fit(X_train, y_train)\n",
    "\n",
    "      pickle_model(\"rf_gridsearch_obj\", grid_search, grid_search.best_params_, param_grid[0])\n",
    "      print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best hyperparams were a max branch depth of 50, a random subset of 70 features for splitting branches, and 150 estimators/trees for random forest. These parameters are the most infuential to model capacity, generalization, and computation. Other parameters like min_samples_split were ommitted from grid search since the default is adequte to recognize patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "if perform_search == True:\n",
    "    model = grid_search.best_estimator_\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred, multioutput='raw_values')\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2_score_values= r2_score(y_test, y_pred, multioutput='raw_values')\n",
    "\n",
    "    # ['Estimated owners', 'Peak CCU', 'Positive', 'Negative', 'Price']\n",
    "    print(\"Test set Mean Squared Error:\", mse)\n",
    "    print(\"Test set Root Mean Squared Error:\", rmse)\n",
    "    print(\"Test set R2 Score:\", r2_score_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model\n",
    "\"\"\" You can download from link in README \"\"\"\n",
    "load_model = True\n",
    "pca_model_filename = 'rf_pca'\n",
    "pca_params = {'max_depth': 50, 'max_features': 70, 'n_estimators': 150}\n",
    "for s in pca_params.values():\n",
    "    pca_model_filename += f'-{s}'\n",
    "try:\n",
    "    if load_model != True:\n",
    "        assert 'Training model'\n",
    "    loaded_model = unpickle_model(pca_model_filename)\n",
    "    model = loaded_model\n",
    "except:\n",
    "    rf_pca = RandomForestRegressor(random_state=42, n_jobs=-1, verbose=1, **pca_params)\n",
    "    model = MultiOutputRegressor(rf_pca, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    pickle_model('rf_pca', model, pca_params)\n",
    "\n",
    "if perform_search == False:\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred, multioutput='raw_values')\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2_score_values= r2_score(y_test, y_pred, multioutput='raw_values')\n",
    "\n",
    "    # ['Estimated owners', 'Peak CCU', 'Positive', 'Negative', 'Price']\n",
    "    print(\"Test set Mean Squared Error:\", mse)\n",
    "    print(\"Test set Root Mean Squared Error:\", rmse)\n",
    "    print(\"Test set R2 Score:\", r2_score_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, fields, field\n",
    "\n",
    "@dataclass\n",
    "class DataMinMax:\n",
    "    data:dict = field(default_factory=dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_labels = [\"Estimated owners\", \"Peak CCU\", \"Positive\", \"Negative\", \"Price\"]\n",
    "calc: DataMinMax = DataMinMax()\n",
    "for i, label in enumerate(predict_labels):\n",
    "    calc.data[label] = {\n",
    "        \"min\": df_orig[label].min(),\n",
    "        \"max\": df_orig[label].max(),\n",
    "        \"r2\": r2_score_values[i],\n",
    "        \"rmse\": rmse[i],\n",
    "    }\n",
    "# owners = df_orig['Estimated owners']\n",
    "# calc.owners = (min())\n",
    "# du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into a DataFrame\n",
    "rows = []\n",
    "for label in predict_labels:\n",
    "\n",
    "    fmt_int = lambda x: f\"{x:,.0f}\"\n",
    "    fmt_float = lambda x, precision=2: f\"{x:,.{precision}f}\"\n",
    "    min_val = calc.data[label][\"min\"]\n",
    "    max_val = calc.data[label][\"max\"]\n",
    "    rmse_val = calc.data[label][\"rmse\"]\n",
    "    r2_val = calc.data[label][\"r2\"]\n",
    "    range_val = max_val - min_val\n",
    "    range_percent = rmse_val * 100\n",
    "    prediction = rmse_val*range_val\n",
    "    rows.append({\n",
    "        \"Metric\": label,\n",
    "        \"Prediction\": fmt_float(prediction, 2),\n",
    "        \"Min\": fmt_float(min_val,2),\n",
    "        \"Max\": fmt_float(max_val),\n",
    "        \"RMSE\": fmt_float(rmse_val, 4),\n",
    "        \"R^2\": fmt_float(r2_val, 4),\n",
    "        \"Range (%)\": fmt_float(range_percent,2),\n",
    "\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df_result = pd.DataFrame(rows)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the R2 Score, which indicates the proportion of variance in the dependent variable that is predictable from the independent variables, the model is able to capture underlying patterns decently for the estimated owners, positive number of reviews, and negative number of reviews. This suggests that relationships between the features and target variables are relatively strong, making them easier to predict.\n",
    "\n",
    "This is logical. Game characteristics like developers, publishers, and categories will directly influence price and peak ccu  counts more so than the other target variables. Since these aren't taken into account during training to avoid too many feature encodings, the correlation between these characteristics makes them harder to predict. **This will help us assign a score to each prediction when defining a success rating.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Rating ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of each predicted parameter is determined by its R² score. Variables that account for a larger portion of their variance in predictions are given greater weights because they are more dependable.\n",
    "\n",
    "\n",
    "The composite score will be calculated based on the R² weights, the accuracy of the predictions, and the sentiment score. High predictions for price and number of negative reveiws will be penalized since a game should maximize their peack ccu, esitmated owners, and number of positive reviews will minimizing cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute weights of each predicted value based on r^2 value\n",
    "r2_weights = r2_score_values / np.sum(r2_score_values) # weights add up to 1\n",
    "\n",
    "# transform input data\n",
    "X_scaled = scaler.transform(X)\n",
    "X_pca = pca.transform(X_scaled)\n",
    "\n",
    "# function that takes a random/chosen game, and computes its success rating\n",
    "def success_rating(random=False, index=None):\n",
    "    if random == True:\n",
    "        index = np.random.randint(0, df.shape[0])\n",
    "\n",
    "    game = X_pca[index]\n",
    "\n",
    "    # collect predicted and actual metrics for the selected game\n",
    "    predictions = model.predict(game.reshape(1,-1))  # ['Estimated owners', 'Peak CCU', 'Positive', 'Negative', 'Price']\n",
    "    actual_metrics = y[index].reshape(1,-1)\n",
    "\n",
    "    # Calculate RMSE for each predicted metric\n",
    "    rmse = np.sqrt(mean_squared_error(actual_metrics, predictions, multioutput='raw_values')).reshape(1,-1)\n",
    "    # normalize the rmse to prevent peak ccu and price from dominating the score (expecting large errors)\n",
    "    rmse = min_max_normalize(rmse)\n",
    "\n",
    "\n",
    "    sentiment_score = df_reviews_with_scores.iloc[index]\n",
    "\n",
    "    # high 'price' and number of 'negative' reviews will be penalized\n",
    "    contributions = r2_weights * rmse\n",
    "    composite_score = np.sum(contributions[0, :3]) - np.sum(contributions[0, 3:-1])  + sentiment_score\n",
    "\n",
    "    info = df_orig.iloc[index]\n",
    "\n",
    "    return predictions, actual_metrics, composite_score, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, score, game_info = success_rating(random=True)\n",
    "#print(\"Predictions: \", game_predictions)\n",
    "#print(\"Actual: \", game_metrics)\n",
    "print(\"Score: \", score)\n",
    "print(game_info.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test how the score differers with different games, lets run 300 iterations of success_rating, and compare the scores to the different metrics that we predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"Score\", \"AppID\", \"Name\", \"Release date\", \"Estimated owners\", \"Peak CCU\",\n",
    "    \"Required age\", \"Price\", \"DLC count\", \"Reviews\", \"Windows\",\n",
    "    \"Mac\", \"Linux\", \"Metacritic score\", \"User score\", \"Positive\",\n",
    "    \"Negative\", \"Achievements\", \"Recommendations\", \"Notes\",\n",
    "    \"Average playtime forever\", \"Average playtime two weeks\",\n",
    "    \"Median playtime forever\", \"Median playtime two weeks\",\n",
    "    \"Developers\", \"Publishers\", \"Categories\", \"Genres\", \"Tags\"\n",
    "]\n",
    "\n",
    "# Create an empty DataFrame with these columns\n",
    "table = pd.DataFrame(columns=columns)\n",
    "\n",
    "# 300 trials to compare scores of different games\n",
    "for i in range(300):\n",
    "    _, _, score, game_info = success_rating(random=True)\n",
    "    game = game_info.to_numpy()\n",
    "\n",
    "    # Convert game_info to a dictionary with matching columns\n",
    "    game_row = dict(zip(columns[1:], game_info))  # Exclude \"Score\" column\n",
    "    game_row[\"Score\"] = score\n",
    "\n",
    "    # Append the dictionary as a new row to the DataFrame\n",
    "    table = pd.concat([table, pd.DataFrame([game_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any duplicates and print the table\n",
    "table = table.drop_duplicates(subset=['AppID'])\n",
    "table = table.sort_values(by='Score', ascending=False)\n",
    "print(table.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrate the results by comparing the score to each predicted metric\n",
    "from matplotlib import pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13,6))\n",
    "fig, (ax3, ax4) = plt.subplots(1, 2, figsize=(13,6))\n",
    "fig, (ax5) = plt.subplots(1, 1, figsize=(6,6))\n",
    "\n",
    "ax1.scatter(table[\"Estimated owners\"], table[\"Score\"], c='red')\n",
    "ax1.set_xlabel('Estimated Owners')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Estimated Owners vs Score')\n",
    "\n",
    "ax2.scatter(table[\"Peak CCU\"], table[\"Score\"], c='blue')\n",
    "ax2.set_xlabel('Peak CCU')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Peak CCU vs Score')\n",
    "\n",
    "ax3.scatter(table[\"Positive\"], table[\"Score\"], c='pink')\n",
    "ax3.set_xlabel('Positive Reviews')\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.set_title('Pos. Reviews vs Score')\n",
    "\n",
    "ax4.scatter(table[\"Negative\"], table[\"Score\"], c='green')\n",
    "ax4.set_xlabel('Negative Reviews')\n",
    "ax4.set_ylabel('Score')\n",
    "ax4.set_title('Neg. Reviews vs Score')\n",
    "\n",
    "ax5.scatter(table[\"Price\"], table[\"Score\"], c='brown')\n",
    "ax5.set_xlabel('Price')\n",
    "ax5.set_ylabel('Score')\n",
    "ax5.set_title('Price vs Score')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1CD4EsTokInoL8rrk7tV9dBVteDf4sZ2l",
     "timestamp": 1731559683460
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
